{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_Assignment9 (1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "lsLT3Ld-1iAl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **امین فتحی - 400722102**"
      ],
      "metadata": {
        "id": "zp37H_4RuNWB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "برای حل این سوال از لینک زیر استفاده شده است : https://colab.research.google.com/github/bentrevett/pytorch-sentiment-analysis/blob/master/1%20-%20Simple%20Sentiment%20Analysis.ipynb#scrollTo=p4uR0OXfZHyW"
      ],
      "metadata": {
        "id": "tCftYPoHuSWs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jCwGY5Uw0r_V"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import random\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load and prepare data**"
      ],
      "metadata": {
        "id": "bG02ldK81-7X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The IMDB sentiment classification dataset consists of 50,000 movie reviews from IMDB users that are labeled as either positive (1) or negative (0). The reviews are preprocessed and each one is encoded as a sequence of word indexes in the form of integers. The words within the reviews are indexed by their overall frequency within the dataset. For example, the integer “2” encodes the second most frequent word in the data. The 50,000 reviews are split into 25,000 for training and 25,000 for testing.\n",
        "\n",
        "Fortunately, the IMDB dataset has already been built in Keras. Since we want to avoid a 50/50 train test split, we will immediately merge the data into data and targets after downloading so we can do an 80/20 split later on.\n"
      ],
      "metadata": {
        "id": "LdycLUWi2Tme"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Loading data...')\n",
        "\n",
        "#Maximum number of words\n",
        "max_features = 20000\n",
        "\n",
        "(train_data, train_label), (test_data, test_label) = imdb.load_data(num_words=max_features)\n",
        "data = np.concatenate((train_data, test_data), axis=0)\n",
        "targets = np.concatenate((train_label, test_label), axis=0)\n",
        "maxlen = 100\n",
        "\n",
        "# The data must be the same length to enter as the input of our recursive neural network.\n",
        "X = pad_sequences(data, maxlen=maxlen)\n",
        "\n",
        "# split data to train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, targets, test_size=0.2)\n",
        "\n",
        "print('x_train shape:', X_train.shape)\n",
        "print('x_test shape:', X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Dpp17FK2uVe",
        "outputId": "8eacf02e-6166-4f36-ca4e-749614b20b0a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n",
            "x_train shape: (40000, 100)\n",
            "x_test shape: (10000, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it is time to build a very simple recursive model. Use the embedding layer as the first layer.You can see more information about this layers in the following link. You are not allowed to use LSTM in this exercise."
      ],
      "metadata": {
        "id": "7FOgp0Ql673y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n",
        "\n",
        "https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html"
      ],
      "metadata": {
        "id": "Fb1pl-BjCc48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters\n",
        "n_epochs = 20\n",
        "lr = 0.0001\n",
        "\n",
        "# Define Loss, Optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = None    # defined after creating model\n",
        "\n",
        "class RNN_Model(nn.Module):\n",
        "  def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "    super(RNN_Model,self).__init__()\n",
        "    self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "    self.rnn = nn.RNN(embedding_dim, hidden_dim,batch_first=True)    \n",
        "    self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "  def forward(self,X):\n",
        "    #x = [batch size, sent len]\n",
        "\n",
        "    embedded = self.embedding(X)\n",
        "    #embedded = [batch size, sent len, emb dim]\n",
        "\n",
        "    output, hidden = self.rnn(embedded)\n",
        "    #output = [batch size, sent len, hid dim]\n",
        "    #hidden = [1, batch size, hid dim]\n",
        "\n",
        "    return self.fc(hidden.squeeze(0))\n",
        "\n"
      ],
      "metadata": {
        "id": "_Fz91EIzQwzu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model = RNN_Model(max_features,200,256,1).to(device)"
      ],
      "metadata": {
        "id": "_PfIByeNOh8i"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(),lr=lr)"
      ],
      "metadata": {
        "id": "sCXEgARORANG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = y_train.reshape((-1,1)).astype('float64')\n",
        "y_test = y_test.reshape((-1,1)).astype('float64')\n",
        "train_dataset = torch.utils.data.TensorDataset(torch.tensor(X_train),torch.tensor(y_train))\n",
        "train_iter = torch.utils.data.DataLoader(train_dataset,batch_size=64)\n",
        "test_dataset = torch.utils.data.TensorDataset(torch.tensor(X_test),torch.tensor(y_test))\n",
        "test_iter = torch.utils.data.DataLoader(test_dataset,batch_size=64)"
      ],
      "metadata": {
        "id": "x2rHYh4sRGYU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float() #convert into float for division \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc.item()"
      ],
      "metadata": {
        "id": "gEk1opFHRJmS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train your model with training data, then report the model result on the test data."
      ],
      "metadata": {
        "id": "rN5AAGx1zTIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train your model\n",
        "train_losses = list()\n",
        "test_losses = list()\n",
        "train_accs = list()\n",
        "test_accs = list()\n",
        "for epoch in range (n_epochs):\n",
        "  train_loss = list()\n",
        "  train_acc = list() \n",
        "  test_loss = list()\n",
        "  test_acc = list() \n",
        "\n",
        "  model.train()\n",
        "  for i, (x, y) in enumerate(train_iter):\n",
        "    optimizer.zero_grad()\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    yprim = model(x)\n",
        "    loss= criterion(yprim, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss.append(loss.item())\n",
        "    train_acc.append(binary_accuracy(yprim,y))\n",
        "  \n",
        "  train_losses.append(sum(train_loss)/len(train_loss))\n",
        "  train_accs.append(sum(train_acc)/len(train_acc))\n",
        "  \n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for i, (X, y) in enumerate(test_iter):\n",
        "      X, y = X.to(device), y.to(device) \n",
        "      yprim = model(X)\n",
        "      loss = criterion(yprim, y)\n",
        "      test_loss.append(loss.item())\n",
        "      test_acc.append(binary_accuracy(yprim,y))\n",
        "    \n",
        "    test_losses.append(sum(test_loss)/len(test_loss))\n",
        "    test_accs.append(sum(test_acc)/len(test_acc))\n",
        "  print(f'train loss: {train_losses[-1]} , train_accuracy: {train_accs[-1]} , test_loss: {test_losses[-1]} , test_accuracy: {test_accs[-1]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-93RlMPpr09",
        "outputId": "139b6717-518b-4db7-864f-9c8fa2adaae0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 0.6265254256805405 , train_accuracy: 0.63875 , test_loss: 0.5599500434810539 , test_accuracy: 0.7202428343949044\n",
            "train loss: 0.5311347952781711 , train_accuracy: 0.73665 , test_loss: 0.5219048845993612 , test_accuracy: 0.7484076433121019\n",
            "train loss: 0.48415013163690457 , train_accuracy: 0.7722 , test_loss: 0.4875741836966748 , test_accuracy: 0.7713972929936306\n",
            "train loss: 0.4424892383428756 , train_accuracy: 0.800575 , test_loss: 0.46574924748210544 , test_accuracy: 0.7886146496815286\n",
            "train loss: 0.4116207946780836 , train_accuracy: 0.8173 , test_loss: 0.4459292485341515 , test_accuracy: 0.7985668789808917\n",
            "train loss: 0.3796816525353468 , train_accuracy: 0.835575 , test_loss: 0.43543341026542626 , test_accuracy: 0.807921974522293\n",
            "train loss: 0.35365962484389313 , train_accuracy: 0.850375 , test_loss: 0.4248165316113178 , test_accuracy: 0.8139928343949044\n",
            "train loss: 0.3328312044329621 , train_accuracy: 0.862425 , test_loss: 0.4197124061523427 , test_accuracy: 0.8131966560509554\n",
            "train loss: 0.314089732323162 , train_accuracy: 0.8725 , test_loss: 0.4158328794447657 , test_accuracy: 0.8146894904458599\n",
            "train loss: 0.3016322778170419 , train_accuracy: 0.877225 , test_loss: 0.41865912201212996 , test_accuracy: 0.8240445859872612\n",
            "train loss: 0.2819168809776413 , train_accuracy: 0.88935 , test_loss: 0.4043601188562027 , test_accuracy: 0.8314092356687898\n",
            "train loss: 0.26549786761825017 , train_accuracy: 0.897975 , test_loss: 0.41783266211782544 , test_accuracy: 0.8323049363057324\n",
            "train loss: 0.25261379441419557 , train_accuracy: 0.903825 , test_loss: 0.4306573012137647 , test_accuracy: 0.8321058917197452\n",
            "train loss: 0.23495058793471618 , train_accuracy: 0.9123 , test_loss: 0.44325068058519806 , test_accuracy: 0.8321058917197452\n",
            "train loss: 0.21791363462968183 , train_accuracy: 0.920125 , test_loss: 0.4450550733808786 , test_accuracy: 0.8349920382165605\n",
            "train loss: 0.2042862763107194 , train_accuracy: 0.925675 , test_loss: 0.4404681249310211 , test_accuracy: 0.8386743630573248\n",
            "train loss: 0.1929809743683496 , train_accuracy: 0.930525 , test_loss: 0.4302337578015939 , test_accuracy: 0.8417595541401274\n",
            "train loss: 0.17539379454680293 , train_accuracy: 0.938575 , test_loss: 0.4344490953074388 , test_accuracy: 0.8434514331210191\n",
            "train loss: 0.1691431760286043 , train_accuracy: 0.939925 , test_loss: 0.42638791966469175 , test_accuracy: 0.8450437898089171\n",
            "train loss: 0.1624944708604913 , train_accuracy: 0.942675 , test_loss: 0.43573164893972377 , test_accuracy: 0.8273288216560509\n"
          ]
        }
      ]
    }
  ]
}